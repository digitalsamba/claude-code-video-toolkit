# RunPod Serverless handler for Wan2.2 Image-to-Video with CHUNKED model layers
#
# This version splits the ~117GB model into multiple ~20-30GB layers to avoid
# registry upload issues with a single massive layer.
#
# Build:
#   docker build -f Dockerfile.chunked --build-arg HF_TOKEN=hf_xxx -t registry/wan-i2v:chunked .
#
# Image size: ~140GB (6 model layers + base)
# Layer sizes: ~8-28GB each (manageable for most registries)
#
# Version: 3.0.0 - Chunked layers for reliable registry push

FROM nvidia/cuda:12.4.1-cudnn-devel-ubuntu22.04

ENV DEBIAN_FRONTEND=noninteractive

# Install Python and system dependencies
RUN apt-get update && apt-get install -y \
    python3.11 \
    python3-pip \
    python3.11-venv \
    python3.11-dev \
    git \
    git-lfs \
    ffmpeg \
    curl \
    build-essential \
    ninja-build \
    && rm -rf /var/lib/apt/lists/* \
    && ln -sf /usr/bin/python3.11 /usr/bin/python3 \
    && ln -sf /usr/bin/python3.11 /usr/bin/python

RUN git lfs install

WORKDIR /app

ENV CUDA_HOME=/usr/local/cuda
ENV PATH="${CUDA_HOME}/bin:${PATH}"
ENV LD_LIBRARY_PATH="${CUDA_HOME}/lib64:${LD_LIBRARY_PATH}"

# Install PyTorch with CUDA 12.4 support
RUN pip3 install --no-cache-dir \
    torch==2.5.1 \
    torchvision==0.20.1 \
    torchaudio==2.5.1 \
    --index-url https://download.pytorch.org/whl/cu124

# Try flash-attn (optional)
RUN pip3 install --no-cache-dir flash-attn --no-build-isolation || \
    echo "flash-attn installation failed, will use SDPA fallback"

# Install LightX2V from source
RUN git clone --depth 1 https://github.com/ModelTC/LightX2V.git /app/lightx2v
WORKDIR /app/lightx2v
RUN pip3 install --no-cache-dir -v .

# Install additional dependencies
RUN pip3 install --no-cache-dir \
    diffusers>=0.30.0 \
    transformers>=4.45.0 \
    accelerate>=0.30.0 \
    safetensors>=0.4.0 \
    einops>=0.7.0 \
    Pillow>=10.0.0 \
    numpy>=1.26.0 \
    imageio>=2.31.0 \
    imageio-ffmpeg>=0.4.8

# LightX2V transitive dependencies (not auto-installed correctly)
RUN pip3 install --no-cache-dir \
    loguru \
    prometheus_client \
    gguf \
    qtorch \
    ftfy \
    decord \
    av \
    opencv-python

# Install SageAttention for optimized video attention (alternative to flash-attn)
RUN pip3 install --no-cache-dir sageattention || echo "sageattention install failed, will try flash-attn"

# Try flash-attn (may need compilation, optional)
RUN pip3 install --no-cache-dir flash-attn --no-build-isolation || \
    echo "flash-attn installation failed, using sageattention fallback"

RUN pip3 install --no-cache-dir \
    runpod>=1.7.0 \
    requests>=2.31.0 \
    boto3>=1.34.0 \
    huggingface_hub>=0.25.0

WORKDIR /app

ENV HF_HOME=/root/.cache/huggingface
ARG HF_TOKEN=""
ENV HF_TOKEN=${HF_TOKEN}

# === CHUNKED MODEL DOWNLOAD ===
# Each RUN creates a separate layer, allowing incremental push/resume

# Layer 1: T5 encoder (~11GB)
RUN python3 -c "\
from huggingface_hub import hf_hub_download; \
import os; \
token = os.environ.get('HF_TOKEN') or None; \
print('Downloading T5 encoder...'); \
hf_hub_download('Wan-AI/Wan2.2-I2V-A14B', 'models_t5_umt5-xxl-enc-bf16.pth', token=token); \
print('T5 encoder done'); \
"

# Layer 2: High noise model shards 1-2 (~19GB)
RUN python3 -c "\
from huggingface_hub import hf_hub_download; \
import os; \
token = os.environ.get('HF_TOKEN') or None; \
print('Downloading high_noise shards 1-2...'); \
hf_hub_download('Wan-AI/Wan2.2-I2V-A14B', 'high_noise_model/diffusion_pytorch_model-00001-of-00006.safetensors', token=token); \
hf_hub_download('Wan-AI/Wan2.2-I2V-A14B', 'high_noise_model/diffusion_pytorch_model-00002-of-00006.safetensors', token=token); \
print('High noise 1-2 done'); \
"

# Layer 3: High noise model shards 3-4 (~19GB)
RUN python3 -c "\
from huggingface_hub import hf_hub_download; \
import os; \
token = os.environ.get('HF_TOKEN') or None; \
print('Downloading high_noise shards 3-4...'); \
hf_hub_download('Wan-AI/Wan2.2-I2V-A14B', 'high_noise_model/diffusion_pytorch_model-00003-of-00006.safetensors', token=token); \
hf_hub_download('Wan-AI/Wan2.2-I2V-A14B', 'high_noise_model/diffusion_pytorch_model-00004-of-00006.safetensors', token=token); \
print('High noise 3-4 done'); \
"

# Layer 4: High noise model shards 5-6 (~16GB)
RUN python3 -c "\
from huggingface_hub import hf_hub_download; \
import os; \
token = os.environ.get('HF_TOKEN') or None; \
print('Downloading high_noise shards 5-6...'); \
hf_hub_download('Wan-AI/Wan2.2-I2V-A14B', 'high_noise_model/diffusion_pytorch_model-00005-of-00006.safetensors', token=token); \
hf_hub_download('Wan-AI/Wan2.2-I2V-A14B', 'high_noise_model/diffusion_pytorch_model-00006-of-00006.safetensors', token=token); \
hf_hub_download('Wan-AI/Wan2.2-I2V-A14B', 'high_noise_model/config.json', token=token); \
hf_hub_download('Wan-AI/Wan2.2-I2V-A14B', 'high_noise_model/diffusion_pytorch_model.safetensors.index.json', token=token); \
print('High noise 5-6 done'); \
"

# Layer 5: Low noise model shards 1-2 (~19GB)
RUN python3 -c "\
from huggingface_hub import hf_hub_download; \
import os; \
token = os.environ.get('HF_TOKEN') or None; \
print('Downloading low_noise shards 1-2...'); \
hf_hub_download('Wan-AI/Wan2.2-I2V-A14B', 'low_noise_model/diffusion_pytorch_model-00001-of-00006.safetensors', token=token); \
hf_hub_download('Wan-AI/Wan2.2-I2V-A14B', 'low_noise_model/diffusion_pytorch_model-00002-of-00006.safetensors', token=token); \
print('Low noise 1-2 done'); \
"

# Layer 6: Low noise model shards 3-4 (~19GB)
RUN python3 -c "\
from huggingface_hub import hf_hub_download; \
import os; \
token = os.environ.get('HF_TOKEN') or None; \
print('Downloading low_noise shards 3-4...'); \
hf_hub_download('Wan-AI/Wan2.2-I2V-A14B', 'low_noise_model/diffusion_pytorch_model-00003-of-00006.safetensors', token=token); \
hf_hub_download('Wan-AI/Wan2.2-I2V-A14B', 'low_noise_model/diffusion_pytorch_model-00004-of-00006.safetensors', token=token); \
print('Low noise 3-4 done'); \
"

# Layer 7: Low noise model shards 5-6 + VAE + misc (~17GB)
RUN python3 -c "\
from huggingface_hub import hf_hub_download; \
import os; \
token = os.environ.get('HF_TOKEN') or None; \
print('Downloading low_noise shards 5-6 + VAE...'); \
hf_hub_download('Wan-AI/Wan2.2-I2V-A14B', 'low_noise_model/diffusion_pytorch_model-00005-of-00006.safetensors', token=token); \
hf_hub_download('Wan-AI/Wan2.2-I2V-A14B', 'low_noise_model/diffusion_pytorch_model-00006-of-00006.safetensors', token=token); \
hf_hub_download('Wan-AI/Wan2.2-I2V-A14B', 'low_noise_model/config.json', token=token); \
hf_hub_download('Wan-AI/Wan2.2-I2V-A14B', 'low_noise_model/diffusion_pytorch_model.safetensors.index.json', token=token); \
hf_hub_download('Wan-AI/Wan2.2-I2V-A14B', 'Wan2.1_VAE.pth', token=token); \
hf_hub_download('Wan-AI/Wan2.2-I2V-A14B', 'configuration.json', token=token); \
print('Low noise 5-6 + VAE done'); \
"

# Layer 8: Tokenizer files (~16MB - tiny layer)
RUN python3 -c "\
from huggingface_hub import hf_hub_download; \
import os; \
token = os.environ.get('HF_TOKEN') or None; \
print('Downloading tokenizer...'); \
hf_hub_download('Wan-AI/Wan2.2-I2V-A14B', 'google/umt5-xxl/tokenizer.json', token=token); \
hf_hub_download('Wan-AI/Wan2.2-I2V-A14B', 'google/umt5-xxl/spiece.model', token=token); \
hf_hub_download('Wan-AI/Wan2.2-I2V-A14B', 'google/umt5-xxl/tokenizer_config.json', token=token); \
hf_hub_download('Wan-AI/Wan2.2-I2V-A14B', 'google/umt5-xxl/special_tokens_map.json', token=token); \
print('Tokenizer done'); \
"

# Copy handler
COPY handler.py /app/handler.py

ENV PYTHONUNBUFFERED=1
ENV LIGHTX2V_PATH=/app/lightx2v
ENV MODEL_BAKED=1

# Health check
RUN python3 -c "\
import torch; \
print(f'PyTorch {torch.__version__}, CUDA available: {torch.cuda.is_available()}'); \
import sys; sys.path.insert(0, '/app/lightx2v'); \
print('LightX2V installed at /app/lightx2v'); \
print('Wan2.2-I2V-A14B model baked into image (chunked layers)'); \
"

CMD ["python3", "-u", "/app/handler.py"]
